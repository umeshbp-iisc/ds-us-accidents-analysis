# -*- coding: utf-8 -*-
"""DS_Project_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18NvYRY4W7SGwZO9onEtOOKnraFTBzRKM
"""

!git clone https://github.com/umeshbp-iisc/ds-us-accidents-analysis.git

# Commented out IPython magic to ensure Python compatibility.
# %cd ds-us-accidents-analysis

!ls

# Mount Google Drive ---
from google.colab import drive
drive.mount('/content/drive')

# Create a folder in your Google Drive to store the dataset
!mkdir -p "/content/drive/MyDrive/us_accidents_data"

from google.colab import files
files.upload()  # ðŸ‘ˆ Upload kaggle.json here

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

#Download dataset from Kaggle directly into Google Drive ---
!kaggle datasets download -d sobhanmoosavi/us-accidents -p "/content/drive/MyDrive/us_accidents_data"

#Unzip dataset inside Google Drive ---
!unzip "/content/drive/MyDrive/us_accidents_data/us-accidents.zip" -d "/content/drive/MyDrive/us_accidents_data"

file_path = '/content/drive/MyDrive/us_accidents_data/US_Accidents_March23.csv'

#Export files in chunk
import pandas as pd
chunk_size = 200000
chunks = []
for chunk in pd.read_csv(file_path, chunksize=chunk_size):
    # Example: select only needed columns to reduce memory
    chunks.append(chunk)
data = pd.concat(chunks, ignore_index=True)
print("âœ… Full dataset loaded in chunks successfully!")
print("Shape:", data.shape)

# Safely convert Start_Time to datetime
data['date1'] = pd.to_datetime(data['Start_Time'], errors='coerce', format='mixed')

# Extract only the date part
data['date2'] = data['date1'].dt.date
data['date'] = pd.to_datetime(data['date2'])

data2 = data[data['date']>='2021-07-01']

# Create a year column
data2['year'] = data2['date'].dt.year

def stratified_by_year_and_severity(df, frac=0.1):
    """
    Stratified sampling by year AND severity.
    Gives best all-round coverage.
    """
    sample_df = df.groupby(['year', 'Severity'], group_keys=False).apply(
        lambda x: x.sample(frac=frac, random_state=42)
    )
    return sample_df.reset_index(drop=True)

sample = stratified_by_year_and_severity(data2, frac=0.05)  # 10% per group
print(sample.head())
print(sample.groupby(['year', 'Severity']).size())

#data3 = data[data['date']>='2021-01-01']
#data3
data3 = sample.copy()
data3

# Sorting data
data3_sorted = data3.sort_values('date1',ascending=True)

#Creating additional features
data4 = data3_sorted[['Severity','Temperature(F)','Wind_Chill(F)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Speed(mph)','Precipitation(in)','Weather_Condition','Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Station','Stop','Traffic_Calming','Traffic_Signal','Turning_Loop','Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight','date']]
data4['month'] = data4['date'].dt.month
data4['dayofweek'] = data4['date'].dt.dayofweek
data4['is_weekend'] = (data4['dayofweek'] >= 5).astype(int)
data4['season'] = ((data4['month']%12 + 3)//3)   # 1=Winter,4=Fall
data4['is_pandemic'] = (data4['date'] >= "2020-03-01").astype(int)

#Importing all necessary libreries
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.pipeline import Pipeline as SkPipeline
from sklearn.feature_selection import SelectKBest, mutual_info_classif
from sklearn.ensemble import StackingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier

from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV
from scipy.stats import uniform
import joblib

RANDOM_STATE = 42
N_JOBS = 1

#converson of binary fearures
bool_cols = ['Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout',
             'Station','Stop','Traffic_Calming','Traffic_Signal','Turning_Loop']

for c in bool_cols:
  if c in data4.columns:
    data4[c] = data4[c].astype(int)

#converson of categorical fearures
cat_time_cols = ['Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight','Weather_Condition']
for c in cat_time_cols:
  if c in data4.columns:
    data4[c] = data4[c].astype(str)

for col in ['Weather_Condition']:
    if col in data4.columns:
        # Ensure column is clean and 1D
        data4[col] = data4[col].astype(str)
        vc = data4[col].dropna()
        if isinstance(vc, pd.Series):
            top10 = vc.value_counts().nlargest(10).index
            data4[col] = data4[col].where(data4[col].isin(top10), other='Other')
        else:
            print(f"âš ï¸ Skipping {col} â€” not a 1D Series.")

#converson of numerical fearures
numeric_cols = ['Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)',
                'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)']

for c in numeric_cols:
  if c in data4.columns:
    data4[c] = pd.to_numeric(data4[c], errors='coerce')

#checking for target
if 'Severity'not in data4.columns:
  raise ValueError("DataFrame must contain a 'Severity' column as the target.")

#choosing of target variable
target_col = 'Severity'

#choosing of features
candidate_features = [
    'Temperature(F)','Wind_Chill(F)','Humidity(%)','Pressure(in)','Visibility(mi)'
    ,'Wind_Speed(mph)','Precipitation(in)','Weather_Condition'
] + bool_cols + ['Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight','date'] + ['month','dayofweek','is_weekend','season','is_pandemic']

#printing of features
features = [c for c in candidate_features if c in data4.columns]
print("Using features:", features)

#train vs validation vs test split
train_start = '2020-01-01'
train_end   = '2021-12-31'     # TRAIN
val_start   = '2022-01-01'     # VALIDATION
val_end     = '2022-12-31'
test_start  = '2023-01-01'     # TEST

train_df = data4[(data4['date'] >= train_start) & (data4['date'] <= train_end)].copy()
val_df   = data4[(data4['date'] >= val_start)  & (data4['date'] <= val_end)].copy()
test_df  = data4[data4['date'] >= test_start].copy()

# Drop date after splitting
train_df = train_df.drop(columns=['date'])
val_df   = val_df.drop(columns=['date'])
test_df  = test_df.drop(columns=['date'])

# Separate X / y
X_train = train_df.drop(columns=[target_col])
y_train = train_df[target_col]

X_val   = val_df.drop(columns=[target_col])
y_val   = val_df[target_col]

X_test  = test_df.drop(columns=[target_col])
y_test  = test_df[target_col]

print(f"Train: {X_train.shape}")
print(f"Validation: {X_val.shape}")
print(f"Test: {X_test.shape}")

#re-checking numerical columns
numeric_features = [c for c in numeric_cols if c in X_train.columns]
cat_features = [
    c for c in X_train.columns
    if c not in numeric_features and c != 'date'
]

#transformation by tyoe of features

numeric_trans = SkPipeline(steps= [
    ('imputer',SimpleImputer(strategy='median')),
    ('scaler',StandardScaler())
])

cat_trans = SkPipeline(steps =[
    ('imputer',SimpleImputer(strategy='most_frequent')),
    ('onehot',OneHotEncoder(handle_unknown='ignore',sparse_output=True))
])

#combining pre-processing
preprocessor = ColumnTransformer(transformers=[
    ('num',numeric_trans,numeric_features),
    ('cat',cat_trans,cat_features)
],sparse_threshold=0.3)

#Feature selection algorithm
selector_estimator = RandomForestClassifier(n_estimators=50, random_state=RANDOM_STATE, n_jobs=N_JOBS)

#model for feature selection
select_from_model = SelectFromModel(estimator=selector_estimator, threshold='median')

#Base Learners
logreg_base = LogisticRegression(
    multi_class='multinomial',
    solver='saga',
    max_iter=1000,
    random_state=RANDOM_STATE
)

rf_base = RandomForestClassifier(
    n_estimators=100,
    max_depth=None,
    random_state=RANDOM_STATE,
    n_jobs=N_JOBS
)

num_classes = y_train.nunique()

xgb_base = XGBClassifier(
    objective='multi:softprob',
    eval_metric='mlogloss',
    num_class=num_classes,
    learning_rate=0.05,
    max_depth=6,
    n_estimators=100,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=RANDOM_STATE,
    n_jobs=N_JOBS
)

#Meta Learner for stacking
stack_meta = LogisticRegression(
    multi_class='multinomial',
    solver='saga',
    max_iter=2000,
    random_state=RANDOM_STATE
)

#Stacking Ensemble
stacked_clf = StackingClassifier(
    estimators=[
        ('logreg', logreg_base),
        ('rf', rf_base),
        ('xgb', xgb_base)
    ],
    final_estimator=stack_meta,
    stack_method='predict_proba',
    passthrough=False,
    n_jobs=N_JOBS
)

#Smote for balaning dataset
from imblearn.over_sampling import SMOTENC

categorical_indices = [X_train.columns.get_loc(col) for col in cat_features]

smote = SMOTENC(
    categorical_features=categorical_indices,
    random_state=42
)

#Preprocess and feature-select once
preprocessed = preprocessor.fit_transform(X_train)
fs_mask = select_from_model.fit(preprocessed, y_train).get_support()
X_selected = preprocessed[:, fs_mask]

#Apply SMOTE once on selected data
X_sm, y_sm = smote.fit_resample(X_selected, y_train)

#New pipeline
pipeline = SkPipeline(steps=[
    ('clf', stacked_clf)
])

#Original Pipeline
pipeline

#Modified pipeline to reduce runtime
pipeline

#K-fold cross validation
kfold = StratifiedKFold(
    n_splits=5,
    shuffle=True,
    random_state=RANDOM_STATE
)

param_grid = {

    # XGBoost tuning
    "clf__xgb__max_depth": [4,6,8],
    "clf__xgb__learning_rate": [0.03, 0.05, 0.1],
    "clf__xgb__subsample": [0.7, 0.8, 1.0],
    "clf__xgb__colsample_bytree": [0.6, 0.8, 1.0],

    # RandomForest tuning
    "clf__rf__n_estimators": [300, 500, 800],
    "clf__rf__max_depth": [None, 10, 20],

    # Meta Learner
    "clf__final_estimator__C": [0.01,0.1,1],
}

#Grid Serach to get best hyper-parameters
search = RandomizedSearchCV(
    estimator=pipeline,
    param_distributions=param_grid,
    n_iter=1,
    scoring="f1_macro",
    cv=kfold,
    verbose=2,
    random_state=RANDOM_STATE,
    n_jobs=N_JOBS
)

#Fitting traning model
search.fit(X_sm, y_sm)

print("\nBest Params:", search.best_params_)
print("Best CV Score (F1-macro):", search.best_score_)

best_model = search.best_estimator_

#Applying model on validation dataset
X_val_t = preprocessor.transform(X_val)
X_val_t = X_val_t[:, fs_mask]
val_pred = best_model.predict(X_val_t)
print("\n==============================")
print("   VALIDATION PERFORMANCE")
print("==============================")

print("Validation Accuracy:", accuracy_score(y_val, val_pred))
print("\nValidation Classification Report:")
print(classification_report(y_val, val_pred, digits=4))

print("\nValidation Confusion Matrix:")
print(confusion_matrix(y_val, val_pred))

#Applying model on test dataset
X_test_t = preprocessor.transform(X_test)
X_test_t = X_test_t[:, fs_mask]
y_pred = best_model.predict(X_test_t)

print("\nTest Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred, digits=4))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

#Computing probablity of classes
y_proba = best_model.predict_proba(X_test_t)

proba_df = pd.DataFrame(
    y_proba,
    columns=[f"Severity_{c}_prob" for c in best_model.named_steps["clf"].classes_]
)

print("\n==============================")
print("PREDICTION PROBABILITIES (Severity)")
print("==============================")
proba_df

#Combinng result with original test dataset
results_df = X_test.copy()
results_df["Actual_Severity"] = y_test.values
results_df["Predicted_Severity"] = y_pred

for i, cls in enumerate(best_model.named_steps["clf"].classes_):
    results_df[f"Prob_Severity_{cls}"] = y_proba[:, i]

print("\n==============================")
print("FINAL RESULTS WITH PROBABILITIES")
print("==============================")
results_df

#Saving the best train model for further use
joblib.dump(best_model, "tuned_logreg_pipeline1.joblib")
print("Saved tuned model to tuned_logreg_pipeline.joblib")

#Getting the list of selected features
print("\n==============================")
print("  FEATURE SELECTION RESULTS")
print("==============================")

# Use the already fitted preprocessor
processed_feature_names = preprocessor.get_feature_names_out()

print(f"\nTotal processed features: {len(processed_feature_names)}")

# Use the already fitted select_from_model
selected_mask = select_from_model.get_support()

selected_features = processed_feature_names[selected_mask]

print(f"Selected features: {len(selected_features)}")
print(selected_features)

#Contributions by meat learners
print("\n==============================")
print("META-LEARNER INPUT CONTRIBUTIONS (stacking level)")
print("==============================")

stack_clf = best_model.named_steps["clf"]
meta_clf = stack_clf.final_estimator_

# Number of meta-features = number of columns in coef matrix
n_meta_features = meta_clf.coef_.shape[1]

meta_feature_names = [f"meta_feature_{i}" for i in range(n_meta_features)]

print("Meta-learner input features:")
print(meta_feature_names)

coef_matrix = meta_clf.coef_

meta_columns = [f"Class_{c+1}" for c in meta_clf.classes_[:]]

contrib_meta_df = pd.DataFrame(
    coef_matrix.T,
    index=meta_feature_names,
    columns=meta_columns
)

contrib_meta_df["mean_abs_contrib"] = contrib_meta_df.abs().mean(axis=1)
contrib_meta_df = contrib_meta_df.sort_values("mean_abs_contrib", ascending=False)

print(contrib_meta_df)

#Contributions by each features towards target
print("\n==============================")
print("LOGISTIC REGRESSION BASE MODEL FEATURE CONTRIBUTIONS")
print("==============================")

logreg = stack_clf.named_estimators_["logreg"]
logreg_coef = logreg.coef_

logreg_columns = [f"Class_{c+1}" for c in logreg.classes_[:]]

contrib_logreg_df = pd.DataFrame(
    logreg_coef.T,
    index=selected_features,
    columns=logreg_columns
)

contrib_logreg_df["mean_abs_contrib"] = contrib_logreg_df.abs().mean(axis=1)
contrib_logreg_df = contrib_logreg_df.sort_values("mean_abs_contrib", ascending=False)

print(contrib_logreg_df)