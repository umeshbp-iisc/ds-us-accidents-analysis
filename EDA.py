# -*- coding: utf-8 -*-
"""DS_Project_Accident_10_NOV.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ijE-4qImyjKP-njmrtzpOozxcKIfTlUX
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/umeshbp-iisc/ds-us-accidents-analysis.git
# %cd ds-us-accidents-analysis

#import functions
import warnings
warnings.filterwarnings('ignore')


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import xgboost as xgb

from google.colab import drive
drive.mount('/content/drive')
file_path = '/content/drive/MyDrive/US_Accidents_March23.csv'

chunk_size = 200000
chunks = []
for chunk in pd.read_csv('/content/drive/MyDrive/US_Accidents_March23.csv', chunksize=chunk_size):
    # Example: select only needed columns to reduce memory
    chunks.append(chunk)
data = pd.concat(chunks, ignore_index=True)
print("Full dataset loaded in chunks successfully!")
print("Shape:", data.shape)

data['start_date'] = pd.to_datetime(data['Start_Time'], format = '%Y-%m-%d %H:%M:%S', errors='coerce')
data['date_start'] = data['start_date'].dt.strftime('%Y-%m-%d')
data['start_time'] = pd.to_datetime(data['start_date'], format= '%H:%M:%S').dt

"""Remove rows before 2020"""

data = data[data['start_date'] >= '2020-01-01']

data['date_start']

#check null values
data.isnull().sum()

"""**Remove Precipitation, Wind Chill, End_lat, End_lng and add '1' column**"""

data.drop(['Precipitation(in)', 'Wind_Chill(F)','End_Lat', 'End_Lng'], axis =1, inplace=True)

data['columm_new_1'] = 1

"""**Clean Zipcode**"""

missing_pincode = data[data['Zipcode'].isnull()]
print(missing_pincode)

data.loc[data['Zipcode'].isnull(), 'Start_Lat']

data.loc[data['Zipcode'].isnull(), 'Start_Lng']

"""**Clean the ZIPCODE column**"""

zipcode_missing = data.loc[
    (data['Start_Lat']> 37.87) & (data['Start_Lat']<37.89) &
    (data['Start_Lng']>-123) & (data['Start_Lng']<-122), 'Zipcode']
zipcode_replace = zipcode_missing.value_counts().idxmax()
data['Zipcode'].fillna(zipcode_replace, inplace=True)

data['Zipcode'].unique()

data = data[~data['Zipcode'].astype(str).str.contains('-',na=False)]
data['Zipcode'] = pd.to_numeric(data['Zipcode'], errors='coerce')

data['Zipcode'].isnull().sum()

data.dtypes

"""**Clean AIRPORT column**"""

data.loc[data['Airport_Code'].isnull(), 'Zipcode']

data.loc[data['Zipcode']== '94710', 'Airport_Code']

def fill_missing_airportcode(row, data):
  if pd.isna(row['Airport_Code']):
    filtered_codes = data.loc[data['Zipcode'] == row['Zipcode'], 'Airport_Code']
    value_counts = filtered_codes.value_counts()
    if not value_counts.empty:
      return value_counts.idxmax()
    else:
      return np.nan
  else:
    return row['Airport_Code']
data['Airport_Code'] = data.apply(lambda row: fill_missing_airportcode(row, data), axis=1)

data['Visibility(mi)']

"""**Assign Num and Cat columns**"""



#check cleaned up
#np.isnan(data_transformed.toarray()).sum()

# Display the first 10 rows of the transformed data by converting it to a dense array
#print(data_transformed.toarray()[:10])
#data.head(10)

#for i, chunk in enumerate(chunks):
  #print(f"Chunk {i}: {chunk.isnull().sum().sum()}")

data['Wind_Direction'].unique()

#univariate analysis
sns.countplot(data=data, x='Severity')
plt.show()

sns.boxplot(x='Roundabout', y='Severity', data=data)
data.groupby('Roundabout')['Severity'].value_counts()

sns.boxplot(x='Sunrise_Sunset', y='Severity', data=data)
data.groupby('Sunrise_Sunset')['Severity'].value_counts()

data['start_date'] = pd.to_datetime(data['Start_Time'], format = '%Y-%m-%d %H:%M:%S', errors='coerce')
data['date_start'] = data['start_date'].dt.strftime('%Y-%m-%d')
data['start_time'] = pd.to_datetime(data['start_date'], format= '%H:%M:%S').dt

#End date and time column
data['end_date'] = pd.to_datetime(data['End_Time'], format = '%Y-%m-%d %H:%M:%S', errors='coerce')
data['date_end'] = data['end_date'].dt.strftime('%Y-%m-%d')
data['end_time'] = pd.to_datetime(data['end_date'], format= '%H:%M:%S').dt

#CALC DIFF (To evaluate impact of time_difference and the severity in seconds)
data['time_difference'] = (data['end_date'] - data['start_date']).dt.total_seconds()

#Plot bivariate analysis to show the relation between severity and time_difference.
sns.boxplot(x='Severity', y='time_difference', data=data)

data['dayofweek'] = data['start_date'].dt.dayofweek
data['day_category'] = data['dayofweek'].apply(lambda x: 'Weekend' if x in [5, 6] else 'Weekday')

groups = data.groupby('day_category')['columm_new_1'].sum().reset_index(name='accident_count')

plt.figure(figsize=(10, 6))
sns.barplot(x='day_category', y='accident_count', hue='day_category', data=groups)
plt.title('Accident Count by Day Category')
plt.xlabel('Day Category')
plt.ylabel('Accident Count')
plt.show()

data['weekday'] = data['start_date'].dt.day_name()
groups = data.groupby('weekday')['columm_new_1'].sum().reset_index(name='accident_count')

plt.figure(figsize=(10, 6))
sns.barplot(x='weekday', y='accident_count', hue='weekday', data=groups)
plt.title('Accident Count by Day of the Week')
plt.xlabel('Weekday')
plt.ylabel('Accident Count')
plt.show()

data = data.dropna(subset=['start_date'])
data['hour'] = data['start_date'].dt.hour.astype(int)

data['year'] = data['start_date'].dt.year

data['month'] = data['start_date'].dt.month
data['month']

#Check month based accidents. Start with month '7' as the first month

groups = data.groupby('month')['columm_new_1'].sum().reset_index(name='accident_count')

groups = groups.sort_values('month')

plt.figure(figsize=(10, 6))
sns.barplot(x='month', y='accident_count', hue='month', data=groups)
plt.title('Accident Count by month of the year')
plt.xlabel('Month of the year')
plt.ylabel('Accident Count')
plt.show()

data['season'] = ((data['month']%12 +3)//3)   # 1=Winter,4=Fall
groups = data.groupby('season')['columm_new_1'].sum().reset_index(name='accident_count')

groups = groups.sort_values('season')

plt.figure(figsize=(10, 6))
sns.barplot(x='season', y='accident_count', hue='season', data=groups)
plt.title('Accident Count by month of the year')
plt.xlabel('Season of the year')
plt.ylabel('Accident Count')
plt.show()

groups = data.groupby(['Astronomical_Twilight', 'month'])['columm_new_1'].sum().reset_index(name='accident_count')
pivot = groups.pivot(index='Astronomical_Twilight', columns='month', values='accident_count')

plt.figure(figsize=(10, 6))
sns.heatmap(pivot, annot=False, cmap='YlGnBu', fmt='.0f')
plt.title('Accident Count by Astronomical_Twilight and month')
plt.xlabel('Astronomical_Twilight')
plt.ylabel('Month')
plt.tight_layout()
plt.show()
plt.figure(figsize=(20, 10))

groups = data.groupby('hour')['columm_new_1'].sum().reset_index(name='accident_count')

plt.figure(figsize=(10, 6))
sns.barplot(x='hour', y='accident_count', hue='hour', data=groups)
plt.title('Accident Count by hour of the day')
plt.xlabel('Hour of the day')
plt.ylabel('Accident Count')
plt.show()

groups = data.groupby(['weekday', 'hour'])['columm_new_1'].sum().reset_index(name='accident_count')
pivot = groups.pivot(index='weekday', columns='hour', values='accident_count')

plt.figure(figsize=(10, 6))
sns.heatmap(pivot, annot=False, cmap='YlGnBu', fmt='.0f')
plt.title('Accident Count by Day of the Week and Hour of the Day')
plt.xlabel('Hour of the day')
plt.ylabel('Weekday')
plt.tight_layout()
plt.show()
plt.figure(figsize=(20, 10))

groups = data.groupby(['Visibility(mi)', 'hour'])['columm_new_1'].sum().reset_index(name='accident_count')
pivot = groups.pivot(index='Visibility(mi)', columns='hour', values='accident_count')
sns.heatmap(pivot, annot=False, cmap='YlGnBu', fmt='.0f')
plt.title('Accident Count by Day of the Week and Hour of the Day')
plt.xlabel('Hour of the day')
plt.ylabel('Visibility')
plt.tight_layout()
plt.show()
plt.figure(figsize=(20, 10))

CA_State = data[data['State'] == 'CA']
city_accidents = CA_State.groupby('City')['columm_new_1'].sum().sort_values(ascending=False).head(10)
city_accidents.plot(kind='bar', color = 'blue')
plt.title('Number of Accidents by city in CA')
plt.xlabel('City in CA')
plt.ylabel('Number of Accidents')
plt.xticks(rotation=90)
plt.figure(figsize=(16, 8))
plt.show()

state_accidents = data.groupby('State')['columm_new_1'].sum().sort_values(ascending=False).head(10)
state_accidents.plot(kind='bar', color = 'blue')
plt.title('Number of Accidents by State')
plt.xlabel('State')
plt.ylabel('Number of Accidents')
plt.xticks(rotation=90)
plt.figure(figsize=(16, 8))
plt.show()

yearly_acc = data.groupby('year')['columm_new_1'].sum().sort_values(ascending=False)
yearly_acc.plot(kind='bar', color = 'blue')
plt.title('Number of Accidents by Year')
plt.xlabel('Year')
plt.ylabel('Number of Accidents')
plt.xticks(rotation=90)
plt.figure(figsize=(16, 8))
plt.show()

"""Weekday and Weekend comparison"""

data['is_weekend'] = data['weekday'].isin(['Saturday', 'Sunday'])
sns.boxplot(x='is_weekend', y='Severity', data=data)

sns.countplot(x='Sunrise_Sunset', hue='Severity', data=data)

sns.countplot(x='Sunrise_Sunset', data=data)

"""Density map"""

import folium
from folium.plugins import HeatMap

sample_data = data[['Start_Lat', 'Start_Lng']].dropna().sample(n=10000)
m = folium.Map(location=[sample_data['Start_Lat'].mean(), sample_data['Start_Lng'].mean()], zoom_start=6)
HeatMap(sample_data[['Start_Lat', 'Start_Lng']].values.tolist()).add_to(m)
m.save("heatmap.html")
#heatmap_data = [[row['Start_lat'], row['Start_lng']] for index, row in sample_data.iterrows()]

# Commented out IPython magic to ensure Python compatibility.
# %cd ds-us-accidents-analysis
!git pull
!git add .
#!git DS_Project_Accident_10_NOV.ipynb
!git config --global user.email "ninadphadnis@iisc.ac.in"
!git config --global user.name "NinadPhadnis"
!git commit -m "Adding intial file"
!git push origin main